{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.data.all import *\n",
    "from local.vision.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/9286.png')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = untar_data(URLs.MNIST_TINY)/'train'\n",
    "items = get_image_files(source)\n",
    "fn = items[0]\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(array(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(x): return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-74b13307e3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-e69650f4afb8>\u001b[0m in \u001b[0;36mneg\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'tuple'"
     ]
    }
   ],
   "source": [
    "neg((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg(array((1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform decorator can make a function work in a flexible way (inputs with different type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def neg(x): return -x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This @Transform looks like you gonna pass the func (neg) in ```__init__``` of class Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent\n",
    "def _neg(x): return -x\n",
    "neg = Transform(_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, -2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg((1,2)) # this is like calling encode function in Transform class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmElEQVR4nGNgGDaAEcbwUrvpLsxocmP5SiyqCv79+3X8xPFH7wWwSJb/c1ZkYGAw+ueBEGOCMXQYHt5nYGD4jKyBiQEPgEsyfvzGwMDAYP1uBxZVPPoMDAwMDJtf4zaI4/493Hbqy28i2UEwdzHikfx/Bp+xN3FLquOz0xifJIp70CVX/8cj+YtBD7ckA4MOHsnPa9FV4wIAY6Uh9HUz4SgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F56ADCA1B50>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = load_image(fn)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0, 164, 239,  26,   0,   0,   0,\n",
       "           0,  96, 236,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  12,  47,   0,   0,   0,   0,\n",
       "           0, 179, 235,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tensor(array(img))\n",
    "t[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,  92,  17, 230,   0,   0,   0,\n",
       "           0, 160,  20,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0, 244, 209,   0,   0,   0,   0,\n",
       "           0,  77,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg(t[12:14]) # negative here doesn't really work, due to type uint8 doesn't allow negative number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0., -164., -239.,\n",
       "          -26.,   -0.,   -0.,   -0.,   -0.,  -96., -236.,   -0.,   -0.,   -0.,\n",
       "           -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.],\n",
       "        [  -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,  -12.,  -47.,\n",
       "           -0.,   -0.,   -0.,   -0.,   -0., -179., -235.,   -0.,   -0.,   -0.,\n",
       "           -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.,   -0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = t.float()\n",
    "neg(t2[12:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization, type notation to work with tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x,m,s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0, 164, 239,  26,   0,   0,   0,\n",
       "           0,  96, 236,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  12,  47,   0,   0,   0,   0,\n",
       "           0, 179, 235,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t2 # to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "          2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "          2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_img = norm(t,127,50)\n",
    "n_img[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.2250)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_img.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def norm(x,m,s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "           0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "           2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "         [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "           2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "         [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.3800,  1.9200,\n",
       "           1.9000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400]]), -2.52)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tupl = (t,1) # assume a image t with label '1'\n",
    "temp = norm(tupl,m=127,s=50) # need keywords for @Transform norm\n",
    "temp[0][12:15],temp[1] # but we don't want label to get transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def norm(x:torch.Tensor,m,s): return (x-m)/s #type notation added: determine which of the tuple to apply tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "           0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "           2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "         [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "           2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "         [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.3800,  1.9200,\n",
       "           1.9000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400]]), 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = norm(tupl,m=127,s=50)\n",
    "temp[0][12:15],temp[1] # fixed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic markers to existing type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??TensorBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTensorImg(TensorBase): pass # Note: why this extra step\n",
    "# fastai wants to add 'semantic markers' to original type to represent 'different kinds of information' (such as to show using show()),\n",
    "# aside from what the original type has to offer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_t = MyTensorImg(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.MyTensorImg, torch.Tensor)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_t),type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "          2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "          2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.3800,  1.9200,\n",
       "          1.9000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(t,m=127,s=50)[12:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "          2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "          2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.3800,  1.9200,\n",
       "          1.9000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(my_t,m=127,s=50)[12:15] # still works, as type notation will allow children of the type specified in the func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def norm2(x:MyTensorImg,m,s): return (x-m)/s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 164., 239.,  26.,   0.,\n",
       "           0.,   0.,   0.,  96., 236.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  12.,  47.,   0.,   0.,\n",
       "           0.,   0.,   0., 179., 235.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   8., 223., 222.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm2(t,m=127,s=50)[12:15] # not working, since torch.Tensor is not the same or children of MyTensorImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "          2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "          2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.3800,  1.9200,\n",
       "          1.9000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm2(my_t,m=127,s=50)[12:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inherit from Transform (to keep a state/attribute for your transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to 'encode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(Transform):\n",
    "    def __init__(self,m,s):\n",
    "        super().__init__()\n",
    "#         store_attr(self,'m,s')\n",
    "        self.m,self.s = m,s\n",
    "    def encodes(self,x:torch.Tensor): return (x-self.m)/self.s\n",
    "    def encodes(self,x: MyTensorImg): return (x-self.m)/self.s\n",
    "    # add other type for encodes here if you want\n",
    "    \n",
    "    def decodes(self,x: MyTensorImg): return (x*self.s)+ self.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent, dynamically. TODO: need testing, since\n",
    "\n",
    "# class Norm(Transform): pass\n",
    "\n",
    "# @Norm\n",
    "# def encodes(self,x:torch.Tensor,m,s): return (x-m)/s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.MyTensorImg, torch.Tensor)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_t),type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "          2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "          2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.3800,  1.9200,\n",
       "          1.9000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Norm(m=127,s=50)\n",
    "f(my_t)[12:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          0.7400,  2.2400, -2.0200, -2.5400, -2.5400, -2.5400, -2.5400, -0.6200,\n",
       "          2.1800, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.3000, -1.6000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,  1.0400,\n",
       "          2.1600, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.3800,  1.9200,\n",
       "          1.9000, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(t)[12:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_t_norm = f(my_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000, 164.0000, 239.0000,  26.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,  96.0000, 236.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,  12.0000,  47.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000, 179.0000, 235.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           8.0000, 223.0000, 222.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.decodes(my_t_norm)[12:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple transform in one go = Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## option 1: use compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).view(3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 0],\n",
       "        [5, 4, 3],\n",
       "        [8, 7, 6]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flip(x,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def flip_img(x:MyTensorImg): return torch.flip(x,[1]) # flip among axis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_norm =Norm(m=127,s=50)\n",
    "f_compose = compose(f_norm,flip_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyTensorImg"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = f_compose(my_t)\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400,  2.1800, -0.6200, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.0200,  2.2400,  0.7400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400,  2.1600,  1.0400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -1.6000, -2.3000, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400,  1.9000,  1.9200, -2.3800, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[12:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-cbfda337d5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# but no decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf_compose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# but no decode\n",
    "f_compose.decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## option 2: use pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that Pipeline will convert everything into Transform, so no need for Transform annotation like in option 1\n",
    "\n",
    "# @Transform: this is no need\n",
    "def flip_img(x:MyTensorImg): return torch.flip(x,[1]) # flip among axis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_norm =Norm(m=127,s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400,  2.1800, -0.6200, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.0200,  2.2400,  0.7400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400,  2.1600,  1.0400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -1.6000, -2.3000, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "        [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400,  1.9000,  1.9200, -2.3800, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "         -2.5400, -2.5400, -2.5400, -2.5400]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline([f_norm,flip_img])\n",
    "temp = p(my_t)\n",
    "temp[12:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyTensorImg"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp) # Note that it keeps the original type, even though flip_img should technically output a torch tensor due to torch,flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyTensorImg"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p.decode(temp)) # decode keeps the original type too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000, 236.0000,  96.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,  26.0000, 239.0000, 164.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000, 235.0000, 179.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,  47.0000,  12.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000, 222.0000, 223.0000,   8.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.decode(temp)[12:15] # note that only f_norm decode is called. flip_img does not have decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyTensorImg"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_t_tuple = (my_t,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400,  2.1800, -0.6200, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.0200,  2.2400,  0.7400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "         [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400,  2.1600,  1.0400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -1.6000, -2.3000, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400],\n",
       "         [-2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400,  1.9000,  1.9200, -2.3800, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400, -2.5400,\n",
       "          -2.5400, -2.5400, -2.5400, -2.5400]]), 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline([f_norm,flip_img])\n",
    "temp = p(my_t_tuple)\n",
    "temp[0][12:15],temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 164., 239.,  26.,   0.,\n",
       "            0.,   0.,   0.,  96., 236.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  12.,  47.,   0.,   0.,\n",
       "            0.,   0.,   0., 179., 235.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   8., 223., 222.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.]]), 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline([f_norm,flip_img],as_item=True) # note again: as_item=True will force the input as a whole object. So this object is gonna be type 'Tuple'\n",
    "temp = p(my_t_tuple)\n",
    "temp[0][12:15],temp[1] #there is no encode(...,o:Tuple), so no transformation is performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify output of function (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def create_image_tensor(x): return tensor(array(load_image(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/9286.png')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = create_image_tensor(fn)\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify/cast output type (NOT WORKING NOW)\n",
    "@Transform\n",
    "def create_image_tensor(x)->TensorImage: return tensor(array(load_image(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = create_image_tensor(fn)\n",
    "type(temp) #TODO: that does not work... it should return local.torch_core.TensorImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "local.torch_core.TensorImage"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorImage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
