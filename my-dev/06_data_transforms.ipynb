{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.data.load import *\n",
    "# from local.transform import *\n",
    "from local.data.core import *\n",
    "from local.data.external import *\n",
    "from local.layers import *\n",
    "from local.torch_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.notebook.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorCategory(TensorBase): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for processing data and basic transforms\n",
    "\n",
    "> Functions for getting, splitting, and labeling data, as well as generic transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get, split, and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most data source creation we need functions to get a list of items, split them in to train/valid sets, and label them. fastai provides functions to make each of these steps easy (especially when combined with `fastai.data.blocks`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll look at functions that *get* a list of items (generally file names).\n",
    "\n",
    "We'll use *tiny MNIST* (a subset of MNIST with just two classes, `7`s and `3`s) for our examples/tests throughout this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [/home/quantran/.fastai/data/mnist_tiny/train/7,/home/quantran/.fastai/data/mnist_tiny/train/3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/quantran/.fastai/data/mnist_tiny')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/quantran/.fastai/data/mnist_tiny/test\n",
      "/home/quantran/.fastai/data/mnist_tiny/models\n",
      "/home/quantran/.fastai/data/mnist_tiny/labels.csv\n",
      "/home/quantran/.fastai/data/mnist_tiny/valid\n",
      "/home/quantran/.fastai/data/mnist_tiny/train\n"
     ]
    }
   ],
   "source": [
    "for i in path.ls(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_files(path, extensions=None, recurse=True, folders=None):\n",
    "    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n",
    "    path = Path(path)\n",
    "    folders=L(folders)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n",
    "            else:                         d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        res = _get_files(path, f, extensions)\n",
    "    return L(res)\n",
    "# this is the best get_files function there is (tested on 1.3 mil imagenet files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most general way to grab a bunch of file names from disk. If you pass `extensions` (including the `.`) then returned file names are filtered by that list. Only those files directly in `path` are included, unless you pass `recurse`, in which case all child folders are also searched recursively. `folders` is an optional list of directories to limit the search to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = get_files(path/'train'/'3', extensions='.png', recurse=False)\n",
    "t7 = get_files(path/'train'/'7', extensions='.png', recurse=False)\n",
    "t  = get_files(path/'train', extensions='.png', recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [/home/quantran/.fastai/data/mnist_tiny/train/7,/home/quantran/.fastai/data/mnist_tiny/train/3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = get_files(path/'train', extensions='.png', recurse=False) # no recursive = not going into directory to look for files with given extensions\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#709) [/home/quantran/.fastai/data/mnist_tiny/train/7/9286.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7686.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8137.png,/home/quantran/.fastai/data/mnist_tiny/train/7/79.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8198.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7663.png,/home/quantran/.fastai/data/mnist_tiny/train/7/9433.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8502.png,/home/quantran/.fastai/data/mnist_tiny/train/7/9270.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7380.png...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eq(len(t), len(t3)+len(t7))\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.jpg', recurse=False)),0)\n",
    "test_eq(len(t), len(get_files(path, extensions='.png', recurse=True, folders='train')))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "test_eq(len(get_files(path/'train'/'3', recurse=False)),346)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, folders=['train', 'test'])),729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to be able to create functions with customized behavior. `fastai.data` generally uses functions named as CamelCase verbs ending in `er` to create these functions. `FileGetter` is a simple example of such a function creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FileGetter(suf='', extensions=None, recurse=True, folders=None):\n",
    "    \"Create `get_files` partial function that searches path suffix `suf`, only in `folders`, if specified, and passes along args\"\n",
    "    def _inner(o, extensions=extensions, recurse=recurse, folders=folders):\n",
    "        return get_files(o/suf, extensions, recurse, folders)\n",
    "    return _inner\n",
    "# function to return a function (similar to partial function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpng = FileGetter(extensions='.png', recurse=False)\n",
    "test_eq(len(t7), len(fpng(path/'train'/'7')))\n",
    "test_eq(len(t), len(fpng(path/'train', recurse=True)))\n",
    "fpng_r = FileGetter(extensions='.png', recurse=True)\n",
    "test_eq(len(t), len(fpng_r(path/'train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_image_files(path, recurse=True, folders=None):\n",
    "    \"Get image files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply `get_files` called with a list of standard image extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(t), len(get_image_files(path, recurse=True, folders='train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ImageGetter(suf='', recurse=True, folders=None):\n",
    "    \"Create `get_image_files` partial function that searches path suffix `suf` and passes along `kwargs`, only in `folders`, if specified.\"\n",
    "    def _inner(o, recurse=recurse, folders=folders): return get_image_files(o/suf, recurse, folders)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `FileGetter`, but for image extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(get_files(path/'train', extensions='.png', recurse=True, folders='3')),\n",
    "        len(ImageGetter(   'train',                    recurse=True, folders='3')(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of functions are used to *split* data into training and validation sets. The functions return two lists - a list of indices or masks for each of training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def RandomSplitter(valid_pct=0.2, seed=None, **kwargs):\n",
    "    \"Create function that splits `items` between train/val with `valid_pct` randomly.\"\n",
    "    def _inner(o, **kwargs):\n",
    "        if seed is not None: torch.manual_seed(seed)\n",
    "        rand_idx = L(int(i) for i in torch.randperm(len(o)))\n",
    "        cut = int(valid_pct * len(o))\n",
    "        return rand_idx[cut:],rand_idx[:cut]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = list(range(30))\n",
    "f = RandomSplitter(seed=42)\n",
    "trn,val = f(src)\n",
    "assert 0<len(trn)<len(src)\n",
    "assert all(o not in val for o in trn)\n",
    "test_eq(len(trn), len(src)-len(val))\n",
    "# test random seed consistency\n",
    "test_eq(f(src)[0], trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _grandparent_idxs(items, name): return mask2idxs(Path(o).parent.parent.name == name for o in items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [path/'train/3/9932.png', path/'valid/7/7189.png', \n",
    "         path/'valid/7/7320.png', path/'train/7/9833.png',  \n",
    "         path/'train/3/7666.png', path/'valid/3/925.png',\n",
    "         path/'train/7/724.png', path/'valid/3/93055.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False, True, True, False, True, False]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Path(o).parent.parent.name == 'train' for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4, 6]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask2idxs([Path(o).parent.parent.name == 'train' for o in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def GrandparentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the grand parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _grandparent_idxs(o, train_name),_grandparent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GrandparentSplitter()\n",
    "test_eq(splitter(items),[[0,3,4,6],[1,2,5,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final set of functions is used to *label* a single item of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parent_label(o, **kwargs): # no need for capital letter function since there's no need for customization through input parameters\n",
    "    \"Label `item` with the parent folder name.\"\n",
    "    return o.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `parent_label` doesn't have anything customize, so it doesn't return a function - you can just use it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/3/9932.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/7/7189.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/7/7320.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/9833.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/3/7666.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/3/925.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/724.png'),\n",
       " PosixPath('/home/quantran/.fastai/data/mnist_tiny/valid/3/93055.png')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '7', '7', '3', '3', '7', '3']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eq(parent_label(items[0]), '3')\n",
    "test_eq(parent_label(\"whatever/the/hell/3/9932.png\"), '3')\n",
    "[parent_label(o) for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def RegexLabeller(pat):\n",
    "    \"Label `item` with regex `pat`.\"\n",
    "    pat = re.compile(pat)\n",
    "    def _inner(o, **kwargs):\n",
    "        res = pat.search(str(o))\n",
    "        assert res,f'Failed to find \"{pat}\" in \"{o}\"'\n",
    "        return res.group(1)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RegexLabeller` is a very flexible function since it handles any regex search of the stringified item. For instance, here's an example the replicates the previous `parent_label` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '7', '7', '3', '3', '7', '3']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = RegexLabeller(r'/(\\d)/')\n",
    "[f(o) for o in items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CategoryMap(CollBase):\n",
    "    \"Collection of categories with the reverse mapping in `o2i`\"\n",
    "    def __init__(self, col, sort=True, add_na=False):\n",
    "        if is_categorical_dtype(col): items = L(col.cat.categories, use_list=True)\n",
    "        else:\n",
    "            if not hasattr(col,'unique'): col = L(col, use_list=True)\n",
    "            # `o==o` is the generalized definition of non-NaN used by Pandas\n",
    "            items = L(o for o in col.unique() if o==o)\n",
    "            if sort: items = items.sorted()\n",
    "        self.items = '#na#' + items if add_na else items # add NaN category/label, e.g. for categorical feature in tabular\n",
    "        self.o2i = defaultdict(int, self.items.val2idx()) if add_na else dict(self.items.val2idx())\n",
    "    def __eq__(self,b): return all_equal(b,self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = CategoryMap([4,2,3,4])\n",
    "test_eq(t, [2,3,4])\n",
    "test_eq(t.o2i, {2:0,3:1,4:2})\n",
    "test_fail(lambda: t.o2i['unseen label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [2,3,4]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t # is a L type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t == t.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#4) [#na#,2,3,4]\n"
     ]
    }
   ],
   "source": [
    "t = CategoryMap([4,2,3,4], add_na=True)\n",
    "print(t)\n",
    "test_eq(t, ['#na#',2,3,4])\n",
    "test_eq(t.o2i, {'#na#':0,2:1,3:2,4:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = CategoryMap(pd.Series([4,2,3,4]), sort=False)\n",
    "test_eq(t, [4,2,3])\n",
    "test_eq(t.o2i, {4:0,2:1,3:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = pd.Series(pd.Categorical(['M','H','L','M'], categories=['H','M','L'], ordered=True))\n",
    "t = CategoryMap(col)\n",
    "test_eq(t, ['H','M','L'])\n",
    "test_eq(t.o2i, {'H':0,'M':1,'L':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [[1],[2, 3]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=L([1],[2,3])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], list)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t,[]),type(sum(t,[])) #wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [[1],[2, 3]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different from this. Also not convert to list\n",
    "t+[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Category(str, ShowTitle): _show_args = {'label': 'category'} #string base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Categorize(Transform):\n",
    "    \"Reversible transform of category string to `vocab` id\"\n",
    "    loss_func,order=CrossEntropyLossFlat(),1\n",
    "    def __init__(self, vocab=None, add_na=False):\n",
    "        self.add_na = add_na\n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab, add_na=add_na)\n",
    "\n",
    "    def setups(self, dsrc):\n",
    "        if self.vocab is None and dsrc is not None: self.vocab = CategoryMap(dsrc, add_na=self.add_na)\n",
    "\n",
    "    def encodes(self, o): \n",
    "        return TensorCategory(self.vocab.o2i[o]) \n",
    "        #vocab is CategoryMap, containing sorted labels (categories) and label2index (o2i)\n",
    "        # return type is TensorCategory, a TensorBase (TODO: why?)\n",
    "        \n",
    "    def decodes(self, o): return Category(self.vocab[o])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "Category.create = Categorize\n",
    "\n",
    "# save 'Categorize' as a function in Category with the name \"create\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Category.create()\n",
    "# This will call 'setups' function in Categorize\n",
    "tds = DataSource(['cat', 'dog', 'cat'], tfms=[cat]) # tfms = [Categorize()]. \n",
    "test_eq(cat.vocab, ['cat', 'dog'])\n",
    "test_eq(cat('cat'), 0) # encode\n",
    "test_eq(cat.decode(1), 'dog')\n",
    "test_stdout(lambda: show_at(tds,2), 'cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.TensorCategory"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cat('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Category.create(add_na=True)\n",
    "tds = DataSource(['cat', 'dog', 'cat'], tfms=[cat])\n",
    "test_eq(cat.vocab, ['#na#', 'cat', 'dog'])\n",
    "test_eq(cat('cat'), 1)\n",
    "test_eq(cat('#na#'), 0)\n",
    "test_eq(cat.decode(2), 'dog')\n",
    "test_stdout(lambda: show_at(tds,2), 'cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicategorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiCategory(L):\n",
    "    def show(self, ctx=None, sep=';', color='black', **kwargs): \n",
    "        return show_title(sep.join(self.map(str)), ctx=ctx, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiCategorize(Categorize):\n",
    "    \"Reversible transform of multi-category strings to `vocab` id\"\n",
    "    loss_func,order=BCEWithLogitsLossFlat(),1\n",
    "    def setups(self, dsrc):\n",
    "        if not dsrc: return\n",
    "        if self.vocab is None:\n",
    "            vals = set()\n",
    "            for b in dsrc: vals = vals.union(set(b))# turn the mess into 1 set\n",
    "            self.vocab = CategoryMap(list(vals))\n",
    "        setattr(dsrc, 'vocab', self.vocab)\n",
    "\n",
    "    def encodes(self, o): return                [self.vocab.o2i  [o_] for o_ in o]\n",
    "    def decodes(self, o): return MultiCategory ([self.vocab[o_] for o_ in o])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "MultiCategory.create = MultiCategorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = MultiCategorize()\n",
    "tds = DataSource([['b', 'c'], ['a'], ['a', 'c']], tfms=[cat])\n",
    "test_eq(cat.vocab, ['a', 'b', 'c'])\n",
    "test_eq(cat(['a', 'c']), [0,2])\n",
    "test_eq(cat([]), [])\n",
    "test_eq(cat.decode([1]), ['b'])\n",
    "test_eq(cat.decode([0,2]), ['a', 'c'])\n",
    "test_stdout(lambda: show_at(tds,2), 'a;c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot1(x, c):\n",
    "    \"One-hot encode `x` with `c` classes.\"\n",
    "    res = torch.zeros(c, dtype=torch.bool)\n",
    "    res[L(x, use_list=None)] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot1(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot1(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot1([0,2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class OneHotEncode(Transform):\n",
    "    \"One-hot encodes targets\"\n",
    "    order=2\n",
    "    def __init__(self, c=None): self.c = c\n",
    "\n",
    "    def setups(self, dsrc):\n",
    "        if self.c is None: self.c = len(L(getattr(dsrc, 'vocab', None)))\n",
    "        if not self.c: warn(\"Couldn't infer the number of classes, please pass a value for `c` at init\")\n",
    "\n",
    "    def encodes(self, o): return TensorCategory(one_hot(o, self.c).bool())\n",
    "    def decodes(self, o): return one_hot_decode(o, None)\n",
    "    \n",
    "# class OneHotEncode(Transform):\n",
    "#     \"One-hot encodes targets and optionally decodes with `vocab`\"\n",
    "#     order=2\n",
    "#     def __init__(self, do_encode=True, vocab=None): self.do_encode,self.vocab = do_encode,vocab\n",
    "\n",
    "#     def setups(self, dsrc):\n",
    "#         if self.vocab is not None:  self.c = len(self.vocab)\n",
    "#         else: self.c = len(L(getattr(dsrc, 'vocab', None)))\n",
    "#         if not self.c: warn(\"Couldn't infer the number of classes, please pass a `vocab` at init\")\n",
    "    \n",
    "#     def encodes(self, o): return one_hot(o, self.c) if self.do_encode else tensor(o).byte() # note that this will return a tensor\n",
    "#     def decodes(self, o): return one_hot_decode(o, self.vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works in conjunction with ` MultiCategorize` or on its own if you have one-hot encoded targets (pass a `vocab` for decoding and `do_encode=False` in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tfm = OneHotEncode(c=3)\n",
    "test_eq(_tfm([0,2]), tensor([True, False, True]))\n",
    "test_eq(_tfm.decode(tensor([0,1,1])), [1,2])\n",
    "test_eq(_tfm.decode(tensor([False,True,True])), [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True, False,  True]), __main__.TensorCategory)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tfm([0,2]),type(_tfm([0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tfm = OneHotEncode(vocab=['a', 'b', 'c'])\n",
    "# tds = DataSource([[1,2], [0], [0, 1]], tfms=[_tfm]) \n",
    "# # you don't put str label ('a','b'...) here since this tfm should be after multicategorize, where str are numericalized (See below)\n",
    "# # in that case, output of multicategorize's vocab will be used for OneHotEncode setups: self.c = len(L(getattr(dsrc, 'vocab', None)))\n",
    "# test_eq(_tfm([0,2]), tensor([1, 0, 1]).byte())\n",
    "# test_eq(_tfm.decode(tensor([0,1,1])), ['b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tfm(1), _tfm([0,2]) # calling encode in OneHotEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do_encode = False => you have to input onehot-ed data (OneHotEncode won't one_hot for you)\n",
    "# _tfm = OneHotEncode(vocab=['a', 'b', 'c'], do_encode=False)\n",
    "# tds = DataSource([[0,1,1], [1,0,0], [1,1,0]], tfms=[_tfm])\n",
    "# test_eq(_tfm([1,0,1]), tensor([1, 0, 1]).byte())\n",
    "# test_eq(_tfm.decode(tensor([0,1,1])), ['b','c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine MultiCategorize and OneHotEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = DataSource([['b', 'c'], ['a'], ['a', 'c'], []], [[MultiCategorize(), OneHotEncode()]])\n",
    "test_eq(tds[1], [tensor([True, False, False])]) # data in tds are already numericaled by multicat and then 1hotencoded to be boolean\n",
    "test_eq(tds[3], [tensor([False, False, False])])\n",
    "test_eq(tds.decode([tensor([False, True, True])]), [['b','c']])\n",
    "test_eq(type(tds[1][0]), TensorCategory)\n",
    "test_stdout(lambda: show_at(tds,2), 'a;c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with passing the vocab\n",
    "tds = DataSource([['b', 'c'], ['a'], ['a', 'c'], []], [[MultiCategorize(vocab=['a', 'b', 'c']), OneHotEncode()]])\n",
    "test_eq(tds[1], [tensor([True, False, False])])\n",
    "test_eq(tds[3], [tensor([False, False, False])])\n",
    "test_eq(tds.decode([tensor([False, True, True])]), [['b','c']])\n",
    "test_eq(type(tds[1][0]), TensorCategory)\n",
    "test_stdout(lambda: show_at(tds,2), 'a;c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tds = DataSource([['b', 'c'], ['a'], ['a', 'c']], [[MultiCategorize(), OneHotEncode()]])\n",
    "# test_eq(tds[1], [tensor([1, 0, 0]).byte()]) # data in tds are already numericaled by multicat and then 1hotencoded\n",
    "# test_eq(tds.decode([tensor([1,1,0])]), [['a','b']])\n",
    "# test_stdout(lambda: show_at(tds,2), 'a;c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_c(dbunch): # get number of classes (labels)\n",
    "    if getattr(dbunch, 'c', False): return dbunch.c\n",
    "    vocab = getattr(dbunch, 'vocab', [])\n",
    "    if len(vocab) > 0 and is_listy(vocab[-1]): vocab = vocab[-1]\n",
    "    return len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end dataset example with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show how to use those functions to grab the mnist dataset in a `DataSource`. First we grab all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split between train and validation depending on the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GrandparentSplitter() \n",
    "splits = splitter(items) # [<indices of train img>,<indices of val img>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#3) [/home/quantran/.fastai/data/mnist_tiny/train/7/9286.png,/home/quantran/.fastai/data/mnist_tiny/train/7/7686.png,/home/quantran/.fastai/data/mnist_tiny/train/7/8137.png],\n",
       " (#3) [/home/quantran/.fastai/data/mnist_tiny/valid/7/9851.png,/home/quantran/.fastai/data/mnist_tiny/valid/7/8510.png,/home/quantran/.fastai/data/mnist_tiny/valid/7/946.png])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,valid = (items[i] for i in splits)\n",
    "train[:3],valid[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs are images that we open and convert to tensors, our targets are labeled depending on the parent directory and are categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def open_img(fn:Path): return Image.open(fn).copy()\n",
    "def img2tensor(im:Image.Image): return TensorImage(array(im)[None])\n",
    "\n",
    "tfms = [[open_img, img2tensor],\n",
    "        [parent_label, Categorize()]]\n",
    "train_ds = DataSource(train, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/quantran/.fastai/data/mnist_tiny/train/7/79.png')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_ds[3]\n",
    "xd,yd = decode_at(train_ds,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]),\n",
       " torch.Size([1, 28, 28]),\n",
       " local.torch_core.TensorImage,\n",
       " local.torch_core.TensorImage)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,xd.shape,type(x),type(xd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(x,xd) \n",
    "# another proof that 'decode' for x does not do anything, probably due to img2tensor (at the end of tfms[0]) already know how to show, \n",
    "# or pipeline tries to maintain type reserves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), '7', __main__.TensorCategory, __main__.Category)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,yd,type(y),type(yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(parent_label(train[3]),yd)\n",
    "test_eq(array(Image.open(train[3])),xd[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??show_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABUCAYAAAA7xZEpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADuElEQVR4nO2bPUgcURSFvyeKYgKihYiFIkRBZAtBRWJhY2ElCOkUo7C1P5W1lQg2ktIqUbEICNoKWhgLfxoVEbSw0GZtFJVFBJkUm5fJXjfZ0czsm4T7wcIy7sw7nD1z993rrvE8D8WnyLWAuKGGCNQQgRoiUEMEaohADRE4N8QYcy8eT8aYT670FLta2OJ53lv73BjzBkgBX13pcZ4QwQfgCthyJSBuhnwEvngO+wkTl17GGFMHnAPvPM87d6UjTgkZAr65NAPiZ8hn1yJiccsYY94D60CN53l3LrXEJSEfgRXXZkBMEhIn4pKQ2KCGCNQQgRoiyNfc/c8V1+Q6qAkRqCECNUSghgjUEIEaIlBDBGqIQA0RqCECNUSghgjUEIHzf2Xm4+bmBoClpSUARkdHAejr6wNgeXkZgLKyslDW04QI8g2ZCzIPSafT7OzsADA2NpZZ+Ieu+/t7AC4uLrKF/fj7xMQEALOzsy9dVuchQShoDXl6egL8umDTkEql2NzczHqtTYAxOd/InwwMDISqURMiKEhC7Lu9sLAAQDKZzDqeLwV/or6+/i/VZaMJEUSSkNPTUwCOjo4Af++QSqV+e87w8DAADQ0NAIyMjADQ3t6e89ypqSkAqqqqQlKdQRMiCHUfcnBwAEBPTw8A19fXOV/X1tYGQFNTEwCJRILx8XEASkpKAKitrQXg6uoq57kbGxsAlJeXv0Tir+g+JAih1hD7aVFUlPE5kUgAMDc3B0BlZSUAdXV1AFRUVDy7RjqdBvyaYa9ZWloKwMzMDPBXyfgjkWzdz87OAGhsbAx8jjWit7cXgO3tbcAvqtaI7u7u10jKhd4yQYhFcwcwODgI+O18Z2cnAOvr60Akt4gmJAixSMja2hpDQ0OA3+7f3t4C0RVPNCHBcJqQ/f19ILORs8moqakB4PLyMsqlQRMSDKdD5unpaSBTN2wydnd3XUrShEgKmpCHhwfAH/utrKwAmSZvayvzm6Hq6upCSnqGJkRQ0IQcHx8DsLq6CvhNYDKZdJ4MiyZEUJCETE5OArC4uJh13I4N7XAoDmhCBJHuVO2wuaurC3g+Ujw5OQFeNjcJEd2pBiGShNhrFhdnl6iWlhYADg8PX3PZsNGEBCGST5n5+XnAHxDbPsV+6SXOaEIEodaQu7vMr0w7OjoAf/q+t7cHQGtr68sVRofWkCCEWkMeHx8BPxn9/f0ANDc3h7lMpGhCBLGYujtCa0gQ8tWQ13/X6R9FEyJQQwRqiEANEaghAjVE8B07pyBEuWWOHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = show_at(train_ds, 3, cmap=\"Greys\", figsize=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ax.title.get_text() in ('3','7')\n",
    "test_fig_exists(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToTensor -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToTensor(Transform):\n",
    "    \"Convert item to appropriate tensor class\"\n",
    "    order = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@docs\n",
    "class Cuda(Transform):\n",
    "    \"Move batch to `device` (defaults to `default_device()`)\"\n",
    "    def __init__(self,device=None):\n",
    "        self.device=default_device() if device is None else device\n",
    "        super().__init__(split_idx=None, as_item=False)\n",
    "    def encodes(self, b): return to_device(b, self.device)\n",
    "    def decodes(self, b): return to_cpu(b)\n",
    "\n",
    "    _docs=dict(encodes=\"Move batch to `device`\", decodes=\"Return batch to CPU\")\n",
    "    \n",
    "    #Note: in pipeline or batch_tfms in DataSource, you can just put in 'to_device' partial function\n",
    "    # but you lose the 'decode' part. Your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Cuda.encodes\" class=\"doc_header\"><code>Cuda.encodes</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Cuda.encodes</code>()\n",
       "\n",
       "Move batch to [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Cuda.encodes, name='Cuda.encodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, like all `Transform`s, `encodes` is called by `tfm()` and `decodes` is called by `tfm.decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1, device='cuda:0'),)\n"
     ]
    }
   ],
   "source": [
    "tfm = Cuda()\n",
    "t = tfm((tensor(1),))\n",
    "print(t)\n",
    "test_eq(*t,1)\n",
    "test_eq(t[0].type(),'torch.cuda.LongTensor' if default_device().type=='cuda' else 'torch.LongTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(Cuda.decodes, name='Cuda.decodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tfm.decode(t)\n",
    "test_eq(*t,1)\n",
    "test_eq(t[0].type(),'torch.LongTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(Transform): \n",
    "    def encodes(self, x): return x \n",
    "    def decodes(self, x): return Int(x) \n",
    "    \n",
    "start = torch.arange(0,50)\n",
    "tds = DataSource(start, [A()])\n",
    "tdl = TfmdDL(tds, after_batch=Cuda, bs=4)\n",
    "test_eq(tdl.device, default_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ByteToFloatTensor -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ByteToFloatTensor(Transform):\n",
    "    \"Transform image to float tensor, optionally dividing by 255 (e.g. for images).\"\n",
    "    order = 20 #Need to run after CUDA if on the GPU\n",
    "    def __init__(self, div=True, div_mask=False, split_idx=None, as_item=True):\n",
    "        super().__init__(split_idx=split_idx,as_item=as_item)\n",
    "        self.div,self.div_mask = div,div_mask\n",
    "\n",
    "    def encodes(self, o:TensorImage): return o.float().div_(255.) if self.div else o.float()\n",
    "    def encodes(self, o:TensorMask ): return o.div_(255.).long() if self.div_mask else o.long() # convert mask to long int\n",
    "    def decodes(self, o:TensorImage): return o.clamp(0., 1.) if self.div else o\n",
    "    # Important: if o does not match with any type (TensorImage or TensorMask => don't encode using this transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TensorImage(tensor(1)) # note that similarly to all transform above, this transform can be executed on 1 single things (doesnt have to be a tuple)\n",
    "tfm = ByteToFloatTensor(as_item=False)\n",
    "ft = tfm(t)\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1), tensor(1))\n",
      "tensor(0.0039)\n"
     ]
    }
   ],
   "source": [
    "t = (TensorImage(tensor(1)),TensorImage(tensor(1)))\n",
    "tfm = ByteToFloatTensor(as_item=True) # note that if you input a Tuple type and as_item=True, the whole Tuple is treated as one single item\n",
    "# and since there is no def encode(...,o:Tuple), no transformation is performed.\n",
    "ft = tfm(t)\n",
    "print(ft)\n",
    "\n",
    "\n",
    "t = TensorImage(tensor(1))\n",
    "tfm = ByteToFloatTensor(as_item=True) # with only one thing (not a tuple), and as_item=True, the type is recognized, so tfm is performed\n",
    "ft = tfm(t)\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (TensorImage(tensor(1)),tensor(2).long(),TensorMask(tensor(3)))\n",
    "tfm = ByteToFloatTensor(as_item=False)\n",
    "ft = tfm(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0039), tensor(2), tensor(3))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.decode(ft[0]) # TODO: isn't decode suppose to switch this back to 255 scale? Why clamp (0,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(local.transform.TensorImage, 'torch.FloatTensor')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfm.decode(ft[0])),tfm.decode(ft[0]).type() # Note: decode will make sure to keep the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ft, [1./255, 2, 3])\n",
    "test_eq(type(ft[0]), TensorImage)\n",
    "test_eq(type(ft[2]), TensorMask)\n",
    "test_eq(ft[0].type(),'torch.FloatTensor')\n",
    "test_eq(ft[1].type(),'torch.LongTensor')\n",
    "test_eq(ft[2].type(),'torch.LongTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def broadcast_vec(dim, ndim, *t, cuda=True):\n",
    "    \"Make a vector broadcastable over `dim` (out of `ndim` total) by prepending and appending unit axes\"\n",
    "    v = [1]*ndim\n",
    "    v[dim] = -1\n",
    "    f = to_device if cuda else noop\n",
    "    return [f(tensor(o).view(*v)) for o in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2],\n",
      "         [3]]], device='cuda:0') torch.Size([1, 3, 1])\n",
      "tensor([[[1]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[3]]], device='cuda:0') torch.Size([3, 1, 1])\n",
      "tensor([[[[1]],\n",
      "\n",
      "         [[2]],\n",
      "\n",
      "         [[3]]]], device='cuda:0') torch.Size([1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "temp = [1,2,3]\n",
    "temp1 = broadcast_vec(1,3,temp)[0]\n",
    "print(temp1,temp1.shape)\n",
    "temp1 = broadcast_vec(0,3,temp)[0]\n",
    "print(temp1,temp1.shape)\n",
    "temp1 = broadcast_vec(1,4,temp)[0] # you can even add new axis (and even extend on that axis?)\n",
    "print(temp1,temp1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@docs\n",
    "class Normalize(Transform):\n",
    "    \"Normalize/denorm batch of `TensorImage`\"\n",
    "    order=99\n",
    "    def __init__(self, mean, std, dim=1, ndim=4, cuda=True): \n",
    "        self.mean,self.std = broadcast_vec(dim, ndim, mean, std, cuda=cuda)\n",
    "        \n",
    "    def encodes(self, x:TensorImage): return (x-self.mean) / self.std\n",
    "    def decodes(self, x:TensorImage): \n",
    "        f = to_cpu if x.device.type=='cpu' else noop\n",
    "        return (x*f(self.std) + f(self.mean))\n",
    "\n",
    "    _docs=dict(encodes=\"Normalize batch\", decodes=\"Denormalize batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfms = [[open_img, img2tensor],\n",
    "#         [parent_label, Categorize()]]\n",
    "# train_ds = DataSource(train, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,  74, 112,  73, 144, 163, 164,\n",
       "           216, 176,  87,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0, 112, 254, 254, 250, 199, 200,\n",
       "           199, 226, 239,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0, 119, 254,  67,  33,   0,   0,\n",
       "             0,  50, 254,  72,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0, 163, 254,  36,   0,   0,   0,\n",
       "             0,  37, 254,  72,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0, 163, 254,  36,   0,   0,   0,\n",
       "             0,  37, 254,  72,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0, 164, 239,  26,   0,   0,   0,\n",
       "             0,  96, 236,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,  12,  47,   0,   0,   0,   0,\n",
       "             0, 179, 235,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             8, 223, 222,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 254, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 254, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 255, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 254,  93,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 254,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            94, 254,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           145, 254,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           146, 255,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            61, 254,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 254, 100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 254, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            55, 241,  61,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]],\n",
       "        dtype=torch.uint8), tensor(1))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 1])\n",
      "torch.Size([1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "mean,std = [0.5]*3,[0.5]*3\n",
    "mean,std = broadcast_vec(1, 4, mean, std)\n",
    "print(mean.shape),print(std.shape)\n",
    "batch_tfms = [Cuda(), ByteToFloatTensor(), Normalize(mean,std)]\n",
    "tdl = TfmdDL(train_ds, after_batch=batch_tfms, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "_dataset_kind",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-e6a056c11003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kwon/fastai_dev/my-dev/local/data/load.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_multiproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kwon/fastai_dev/my-dev/local/data/load.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai_dev/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseDataLoaderIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai_dev/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_collation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_collation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kwon/fastai_dev/my-dev/local/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'_xtra'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xtra\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xtra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xtra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: _dataset_kind"
     ]
    }
   ],
   "source": [
    "x,y  = tdl.one_batch()\n",
    "xd,yd = tdl.after_batch.decode((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(x.type(), 'torch.cuda.FloatTensor' if default_device().type=='cuda' else 'torch.FloatTensor')\n",
    "test_eq(xd.type(), 'torch.FloatTensor')\n",
    "test_eq(type(x), TensorImage)\n",
    "test_eq(type(y), TensorCategory)\n",
    "assert x.mean()<0.0\n",
    "assert x.std()>0.5\n",
    "assert 0<xd.mean()/255.<1\n",
    "assert 0<xd.std()/255.<0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.vision.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdl.show_batch((x,y), figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = torch.add(x,0),torch.add(y,0) #Lose type of tensors (to emulate predictions)\n",
    "test_ne(type(x), TensorImage)\n",
    "tdl.show_batch((x,y), figsize=(4,4)) #Check that types are put back by dl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make the above check a proper test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 01a_torch_core.ipynb.\n",
      "Converted 02_script.ipynb.\n",
      "Converted 03_dataloader.ipynb.\n",
      "Converted 04_transform.ipynb.\n",
      "Converted 05_data_core.ipynb.\n",
      "Converted 06_data_transforms.ipynb.\n",
      "Converted 07_vision_core.ipynb.\n",
      "Converted 08_pets_tutorial.ipynb.\n",
      "Converted 09_vision_augment.ipynb.\n",
      "Converted 11_layers.ipynb.\n",
      "Converted 11a_vision_models_xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 14_callback_schedule.ipynb.\n",
      "Converted 15_callback_hook.ipynb.\n",
      "Converted 16_callback_progress.ipynb.\n",
      "Converted 17_callback_tracker.ipynb.\n",
      "Converted 18_callback_fp16.ipynb.\n",
      "Converted 19_callback_mixup.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_tutorial_imagenette.ipynb.\n",
      "Converted 22_vision_learner.ipynb.\n",
      "Converted 23_tutorial_transfer_learning.ipynb.\n",
      "Converted 30_text_core.ipynb.\n",
      "Converted 31_text_data.ipynb.\n",
      "Converted 32_text_models_awdlstm.ipynb.\n",
      "Converted 33_text_models_core.ipynb.\n",
      "Converted 34_callback_rnn.ipynb.\n",
      "Converted 35_tutorial_wikitext.ipynb.\n",
      "Converted 36_text_models_qrnn.ipynb.\n",
      "Converted 40_tabular_core.ipynb.\n",
      "Converted 41_tabular_model.ipynb.\n",
      "Converted 42_tabular_rapids.ipynb.\n",
      "Converted 50_data_block.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_index.ipynb.\n",
      "Converted 95_utils_test.ipynb.\n",
      "Converted 96_data_external.ipynb.\n",
      "Converted notebook2jekyll.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
